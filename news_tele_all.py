import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import os
import json
from datetime import datetime, timedelta

# ë¸Œë¼ìš°ì € íƒ­ ì„¤ì • (ì¹´í†¡ ë²„ì „ ì œëª© ê·¸ëŒ€ë¡œ ìœ ì§€)
st.set_page_config(page_title="ì„±í›ˆ's News Monitor by Telegram", page_icon="ğŸ“°")

# --- [1] ì‚¬ìš©ì ì„¤ì • (Secrets í™œìš©) ---
TELEGRAM_TOKEN = st.secrets.get("TELEGRAM_TOKEN", "")
MY_CHAT_ID = st.secrets.get("TELEGRAM_CHAT_ID", "") # ê¸°ë³¸ ë‚´ ID
NAVER_CLIENT_ID = st.secrets.get("NAVER_ID", "")
NAVER_CLIENT_SECRET = st.secrets.get("NAVER_SECRET", "")

# --- [2] ë³´ì¡° í•¨ìˆ˜ (ì¹´í†¡ ë²„ì „ ë¡œì§ ê·¸ëŒ€ë¡œ ì´ì‹) ---

def send_telegram(msg, target_id):
    """ì¹´í†¡ send_kakao_to_me ëŒ€ì‹  í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡"""
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
        payload = {"chat_id": target_id, "text": msg, "parse_mode": "HTML"}
        requests.post(url, json=payload)
    except: pass

def get_media_by_domain(url):
    domain_map = {'livesnews.com': 'ë¼ì´ë¸Œë‰´ìŠ¤', 'hinews.kr': 'í•˜ì´ë‰´ìŠ¤', 'mdtoday.co.kr': 'ë©”ë””ì»¬íˆ¬ë°ì´', 'sjbnews.com': 'ìƒˆì „ë¶ì‹ ë¬¸', 'jeonmin.co.kr': 'ì „ë¯¼ì¼ë³´', 'beopbo.com': 'ë²•ë³´ì‹ ë¬¸', 'medicalworldnews.co.kr': 'ë©”ë””ì»¬ì›”ë“œë‰´ìŠ¤', 'kmedinfo.co.kr': 'í•œêµ­ì˜í•™ì •ë³´ì—°êµ¬ì›'}
    low_url = url.lower()
    for domain, name in domain_map.items():
        if domain in low_url: return name
    return None

def shorten_url(url):
    if not url: return ""
    bad_domains = ['sjbnews.com', 'jeonmin.co.kr', 'mdtoday.co.kr', 'hinews.kr', 'livesnews.com']
    if any(d in url.lower() for d in bad_domains): return url
    try:
        res = requests.get(f"https://is.gd/create.php?format=simple&url={url}", timeout=3.0)
        if res.status_code == 200: return res.text.strip()
    except: pass
    return url

def get_real_info(url, title_text=""):
    real_media, real_date = "ë„¤ì´ë²„/daum/google", ""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=5.0)
        if res.status_code == 200:
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            if "naver.com" in url:
                m_press = soup.find('meta', property='og:article:author') or soup.find('meta', {'name':'twitter:creator'})
                if m_press: real_media = m_press.get('content', '').split('|')[0].strip()
            elif "daum.net" in url:
                m_press = soup.find('meta', property='article:media_name')
                if m_press: real_media = m_press.get('content', '').strip()
            if real_media == "ë„¤ì´ë²„/daum/google":
                d_name = get_media_by_domain(url)
                if d_name: real_media = d_name
                else:
                    meta_site = soup.find('meta', property='og:site_name')
                    if meta_site:
                        name = meta_site.get('content', '').strip()
                        if name and name not in ['ë„¤ì´ë²„ ë‰´ìŠ¤', 'ë‹¤ìŒë‰´ìŠ¤', 'Google News', 'Google', 'ë„¤ì´ë²„']: real_media = name
            patterns = [r'(\d{4}[-./]\d{2}[-./]\d{2}).{0,50}?(\d{2}:\d{2})', r'(?:ìŠ¹ì¸|ë°œí–‰|ë“±ë¡|ì…ë ¥|ìˆ˜ì •).*?(\d{4}[-./]\d{2}[-./]\d{2}).{0,100}?(\d{2}:\d{2})']
            for p in patterns:
                m = re.search(p, res.text, re.DOTALL)
                if m:
                    real_date = f"{m.group(1).replace('.','-').replace('/','-')} | {m.group(2)}"
                    break
    except: pass
    if real_media == "ë„¤ì´ë²„/daum/google" and " - " in title_text:
        maybe = title_text.split(" - ")[-1].strip()
        if maybe not in ['ë„¤ì´ë²„ ë‰´ìŠ¤', 'ë‹¤ìŒë‰´ìŠ¤', 'Google News']: real_media = maybe
    if "." in real_media: real_media = real_media.replace(".", "â€¤")
    return real_media, real_date

def parse_api_date(date_str):
    if not date_str: return "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
    try:
        if "," in date_str: dt = datetime.strptime(date_str[:25].strip(), "%a, %d %b %Y %H:%M:%S") + timedelta(hours=9)
        else: dt = datetime.fromisoformat(date_str.replace('Z', '+00:00')) + timedelta(hours=9)
        return dt.strftime("%Y-%m-%d | %H:%M")
    except: return "ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜"

def create_report(keywords, days, target_id):
    final_items = []
    now_korea = datetime.now() + timedelta(hours=9)
    cutoff = (now_korea - timedelta(days=days)).replace(hour=0, minute=0, second=0, microsecond=0)
    junk_keywords = ['ë¶€ê³ ', 'ê²Œì‹œíŒ', 'ì¸ì‚¬', 'í¬í† ', 'ì•Œë¦¼', 'ë™ì •', 'í™”ë³´']

    for kw in keywords:
        search_kw = kw if re.sub(r'\s+', '', kw).isalpha() else f'"{kw}"'
        headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
        r = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params={"query": search_kw, "display": 100, "sort": "date"})
        raw = [{"title": BeautifulSoup(i["title"], "html.parser").get_text(), "url": i["link"], "api_date": i.get("pubDate")} for i in r.json().get("items", [])]
        
        try:
            gr = requests.get(f"https://news.google.com/rss/search?q={search_kw}", timeout=10)
            soup = BeautifulSoup(gr.text, "xml")
            raw += [{"title": item.find("title").text, "url": item.find("link").text, "api_date": item.find("pubDate").text} for item in soup.select("item")]
        except: pass

        for item in raw:
            try:
                clean_title = re.sub(r'\s*-\s*[^-]+$', '', item['title']).strip().replace("...", "").replace("â€¦", "").strip()
                is_dup = False
                for x in final_items:
                    if clean_title[:20] == x['title'][:20] or item['url'] in x['url'] or x['url'] in item['url']:
                        is_dup = True; break
                if is_dup: continue
                if "news.google.com" not in item['url'] and kw.lower() not in item['title'].lower(): continue
                if any(junk in item['title'] for junk in junk_keywords): continue

                api_date = parse_api_date(item['api_date'])
                dt_obj = datetime.strptime(api_date.split(" | ")[0], "%Y-%m-%d")
                if dt_obj >= cutoff:
                    media, real_date = get_real_info(item['url'], item['title'])
                    item['media'], item['date'], item['sort_key'], item['title'] = media, (real_date if real_date else api_date), (real_date if real_date else api_date), clean_title
                    final_items.append(item)
            except: continue

    final_items.sort(key=lambda x: x['sort_key'], reverse=True)
    now_str = now_korea.strftime('%Y-%m-%d | %H:%M')
    
    if len(final_items) > 0:
        # [í…”ë ˆê·¸ë¨ í—¤ë” ì–‘ì‹ - ì„±í›ˆë‹˜ ì¹´í†¡ ë²„ì „ê³¼ 100% ë™ì¼í•˜ê²Œ í–‰ ë‚˜ëˆ”]
        report_header = (
            f"=== ì–¸ë¡  ë‰´ìŠ¤ ê²€ìƒ‰ ===\n"
            f"ğŸ¯ ê²€ìƒ‰ ë‹¨ì–´ : {', '.join(keywords)}\n"
            f"ğŸ—“ï¸ ê²€ìƒ‰ ì‹œê°„ : {now_str}\n"
            f"ğŸ—“ï¸ ê²€ìƒ‰ ê¸°ê°„ : {days}ì¼\n"
            f"ğŸ“ í•´ë‹¹ ê¸°ì‚¬ : ì´ {len(final_items)}ê±´"
        )
        send_telegram(report_header, target_id)

        # [ê¸°ì‚¬ ëª©ë¡ ì–‘ì‹ - í–‰ ë‚˜ëˆ” ì ìš©]
        for idx, it in enumerate(final_items, 1):
            msg = (
                f"[{idx}] {it['title']}\n"
                f"ğŸ—“ï¸ ë°œí–‰ì‹œê°„: {it['date']}\n"
                f"ğŸ“° ì–¸ë¡ ì‚¬: {it['media']}\n"
                f"ğŸ”— ë§í¬: {shorten_url(it['url'])}\n"
            )
            send_telegram(msg, target_id)
            
    return {"keywords": ", ".join(keywords), "time": now_str, "days": days, "count": len(final_items)}

# --- [3] ë©”ì¸ UI (ì„±í›ˆë‹˜ ì¹´í†¡ ë²„ì „ UI ê·¸ëŒ€ë¡œ ìœ ì§€) ---
st.markdown("""<div style="text-align: center;"><h3 style="margin-bottom: 0px;">ğŸ¯ News Monitor (í…”ë ˆê·¸ë¨)</h3><p style="font-size: 13px; color: grey; margin-top: 5px;">Copyright by <span style="color: #1E90FF; font-weight: bold;">ì„±í›ˆ</span></p></div>""", unsafe_allow_html=True)
st.write("")

query_id = st.query_params.get("id", MY_CHAT_ID)

with st.form("search_form"):
    # ì¹œêµ¬ ID ê¸°ëŠ¥ì„ ìœ„í•´ ì…ë ¥ì°½ë§Œ ì¶”ê°€
    target_id_input = st.text_input("ë©”ì‹œì§€ ë°›ì„ í…”ë ˆê·¸ë¨ ID", value=query_id)
    kw_input = st.text_input("í‚¤ì›Œë“œ(ì‰¼í‘œ êµ¬ë¶„)", placeholder="ì˜ˆ: ì˜¬íƒ€ì´íŠ¸, altite")
    day_input = st.slider("ê²€ìƒ‰ ê¸°ê°„ (ì¼)", 1, 100, 1)
    submit_button = st.form_submit_button("ë‰´ìŠ¤ ê²€ìƒ‰ ë° í…”ë ˆê·¸ë¨ ì „ì†¡")

if submit_button and kw_input:
    st.query_params["id"] = target_id_input
    with st.spinner('ë‰´ìŠ¤ ìˆ˜ì§‘ ä¸­...'):
        report = create_report([k.strip() for k in kw_input.split(",")], day_input, target_id_input)
        
        if report['count'] > 0:
            st.success(f"âœ… ì´ {report['count']}ê±´ ë‰´ìŠ¤, í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ!")
            st.balloons()
        else:
            st.warning("âš ï¸ ê²€ìƒ‰ëœ ë‰´ìŠ¤ X, í…”ë ˆê·¸ë¨ ì „ì†¡ X")
            
        st.markdown("---")
        st.markdown(f"### ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½")
        # [ì„±í›ˆë‹˜ ìš”ì²­ ì–‘ì‹ ê·¸ëŒ€ë¡œ]
        st.info(f"""
        ğŸ¯ **ê²€ìƒ‰ ë‹¨ì–´** : {report['keywords']}  
        ğŸ—“ï¸ **ê²€ìƒ‰ ì‹œê°„** : {report['time']}  
        ğŸ—“ï¸ **ê²€ìƒ‰ ê¸°ê°„** : {report['days']}ì¼  
        ğŸ“ **í•´ë‹¹ ê¸°ì‚¬** : ì´ {report['count']}ê±´
        """)
        st.markdown("---")
